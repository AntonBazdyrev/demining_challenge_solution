{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3bcd8d4",
   "metadata": {
    "_cell_guid": "398bec4d-1129-45c6-8577-c2bdafef638e",
    "_uuid": "99fab65b-e04d-4c0f-bc0e-2255cc1f7ff0",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-27T12:06:02.199040Z",
     "iopub.status.busy": "2025-07-27T12:06:02.198783Z",
     "iopub.status.idle": "2025-07-27T12:07:58.641297Z",
     "shell.execute_reply": "2025-07-27T12:07:58.640476Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 116.448184,
     "end_time": "2025-07-27T12:07:58.642608",
     "exception": false,
     "start_time": "2025-07-27T12:06:02.194424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "# !pip install --no-index --no-deps /kaggle/input/predetectron-31-wheel/pycocotools-2.0.6-cp310-cp310-linux_x86_64.whl\n",
    "!pip install --no-index --no-deps /kaggle/input/mmdetectron-31-wheel/torch-1.12.1+cu116-cp310-cp310-linux_x86_64.whl\n",
    "!pip install --no-index --no-deps /kaggle/input/mmdetectron-31-wheel/torchvision-0.13.1+cu116-cp310-cp310-linux_x86_64.whl\n",
    "!pip install --no-index --no-deps /kaggle/input/mmdetectron-31-wheel/mmcv-2.0.1-cp310-cp310-manylinux1_x86_64.whl \n",
    "!pip install --no-index --no-deps /kaggle/input/mmdetectron-31-wheel/openmim-0.3.9-py2.py3-none-any.whl\n",
    "!pip install --no-index --no-deps /kaggle/input/mmdetectron-31-wheel/mmengine-0.7.4-py3-none-any.whl\n",
    "!pip install --no-index --no-deps /kaggle/input/mmdetectron-31-wheel/addict-2.4.0-py3-none-any.whl\n",
    "!pip install --no-index --no-deps /kaggle/input/ai-jam-inference/yapf-0.40.1-py3-none-any.whl\n",
    "!pip install --no-index --no-deps /kaggle/input/ai-jam-inference/terminaltables-3.1.10-py2.py3-none-any.whl\n",
    "!pip install --no-index --no-deps /kaggle/input/ai-jam-inference/setuptools-69.5.1-py3-none-any.whl\n",
    "!pip install --no-index --no-deps /kaggle/input/mmpretrain/einops-0.6.1-py3-none-any.whl\n",
    "!pip install --no-index --no-deps /kaggle/input/mmpretrain/mat4py-0.5.0-py2.py3-none-any.whl\n",
    "!pip install --no-index --no-deps /kaggle/input/mmpretrain/ordered_set-4.1.0-py3-none-any.whl\n",
    "!pip install --no-index --no-deps /kaggle/input/mmpretrain/model_index-0.1.11-py3-none-any.whl\n",
    "!pip install --no-index --no-deps /kaggle/input/mmpretrain/modelindex-0.0.2-py3-none-any.whl\n",
    "!pip install --no-index --no-deps /kaggle/input/mmpretrain/mmpretrain-1.0.0rc8-py2.py3-none-any.whl\n",
    "\n",
    "!pip install --no-index --no-deps /kaggle/input/ai-jam-inference/ensemble_boxes-1.0.9-py3-none-any.whl\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00e6fe98",
   "metadata": {
    "_cell_guid": "62483bbf-baf6-47d9-aff6-4b4995be8b0d",
    "_uuid": "a7e4b9bb-fd74-4a29-8204-0791bfd82228",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-27T12:07:58.649996Z",
     "iopub.status.busy": "2025-07-27T12:07:58.649741Z",
     "iopub.status.idle": "2025-07-27T12:08:02.306445Z",
     "shell.execute_reply": "2025-07-27T12:08:02.305549Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 3.66195,
     "end_time": "2025-07-27T12:08:02.307927",
     "exception": false,
     "start_time": "2025-07-27T12:07:58.645977",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --no-index --no-deps /kaggle/input/ai-jam-inference/mmdet-3.3.0-py3-none-any.whl\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7b025cd",
   "metadata": {
    "_cell_guid": "215c5235-a38e-4d86-92d0-aa0300c16add",
    "_uuid": "bda2400a-ad40-447d-a558-40e770496b87",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-27T12:08:02.315212Z",
     "iopub.status.busy": "2025-07-27T12:08:02.314955Z",
     "iopub.status.idle": "2025-07-27T12:08:04.012210Z",
     "shell.execute_reply": "2025-07-27T12:08:04.011398Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1.702381,
     "end_time": "2025-07-27T12:08:04.013706",
     "exception": false,
     "start_time": "2025-07-27T12:08:02.311325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 1.12.1+cu116 cuda: True\n",
      "mmdetection: 3.3.0\n",
      "mmcv: 2.0.1\n",
      "mmengine: 0.7.4\n"
     ]
    }
   ],
   "source": [
    "# Check Pytorch installation\n",
    "import torch, torchvision\n",
    "print(\"torch version:\",torch.__version__, \"cuda:\",torch.cuda.is_available())\n",
    "# Check MMDetection installation\n",
    "import mmdet\n",
    "print(\"mmdetection:\",mmdet.__version__)\n",
    "# Check mmcv installation\n",
    "import mmcv\n",
    "print(\"mmcv:\",mmcv.__version__)\n",
    "# Check mmengine installation\n",
    "import mmengine\n",
    "print(\"mmengine:\",mmengine.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "075617e5",
   "metadata": {
    "_cell_guid": "8db57868-8799-4f52-995c-12061025a800",
    "_uuid": "6c1b1262-b894-4ef6-9213-52022f321003",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-27T12:08:04.021073Z",
     "iopub.status.busy": "2025-07-27T12:08:04.020731Z",
     "iopub.status.idle": "2025-07-27T12:08:04.023797Z",
     "shell.execute_reply": "2025-07-27T12:08:04.023102Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.00798,
     "end_time": "2025-07-27T12:08:04.025059",
     "exception": false,
     "start_time": "2025-07-27T12:08:04.017079",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We download the pre-trained checkpoints for inference and finetuning.\n",
    "#!mim download mmdet --config rtmdet_tiny_8xb32-300e_coco --dest /kaggle/working/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04969624",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T12:08:04.032009Z",
     "iopub.status.busy": "2025-07-27T12:08:04.031779Z",
     "iopub.status.idle": "2025-07-27T12:08:08.438095Z",
     "shell.execute_reply": "2025-07-27T12:08:08.437055Z"
    },
    "papermill": {
     "duration": 4.41128,
     "end_time": "2025-07-27T12:08:08.439671",
     "exception": false,
     "start_time": "2025-07-27T12:08:04.028391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --no-index \\\n",
    "            --find-links=/kaggle/input/prepare-yolo-env \\\n",
    "            ultralytics==8.3.169\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88508e84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T12:08:08.447034Z",
     "iopub.status.busy": "2025-07-27T12:08:08.446787Z",
     "iopub.status.idle": "2025-07-27T12:08:46.686877Z",
     "shell.execute_reply": "2025-07-27T12:08:46.686201Z"
    },
    "papermill": {
     "duration": 38.245444,
     "end_time": "2025-07-27T12:08:46.688500",
     "exception": false,
     "start_time": "2025-07-27T12:08:08.443056",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/albumentations/check_version.py:49: UserWarning: Error fetching version info <urlopen error [Errno -3] Temporary failure in name resolution>\n",
      "  data = fetch_version_info()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
      "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "\n",
    "# MMDetection imports\n",
    "from mmdet.apis import DetInferencer\n",
    "\n",
    "# YOLO and ensemble imports\n",
    "from ultralytics import YOLO\n",
    "from ensemble_boxes import weighted_boxes_fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1496ed30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T12:08:46.696558Z",
     "iopub.status.busy": "2025-07-27T12:08:46.696137Z",
     "iopub.status.idle": "2025-07-27T12:08:46.700520Z",
     "shell.execute_reply": "2025-07-27T12:08:46.699913Z"
    },
    "papermill": {
     "duration": 0.009621,
     "end_time": "2025-07-27T12:08:46.701787",
     "exception": false,
     "start_time": "2025-07-27T12:08:46.692166",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Global model configuration\n",
    "MODEL_CONFIG = '/kaggle/input/ai-jam-inference/rtmdet_tiny_1xb4-20e_demine_test (1).py'\n",
    "CHECKPOINT = '/kaggle/input/ai-jam-inference/best_coco_bbox_mAP_epoch_19_semifixedv2.pth'\n",
    "\n",
    "# Multiple YOLO models\n",
    "YOLO_MODEL_PATHS = [\n",
    "    \"/kaggle/input/demining-models/yolo11_onlylabeletrain_atleast1boxcrop_underfited/yolo11_onlylabeletrain_atleast1boxcrop_underfited/best.pt\",\n",
    "    \"/kaggle/input/demining-models/yolo11_onlylabeletrain_atleast1boxcrop_middlefited/yolo11_onlylabeletrain_atleast1boxcrop_middlefited/last.pt\",\n",
    "    \"/kaggle/input/demining-models/yolo11_onlylabeletrain_atleast1boxcrop_underfited/yolo11_onlylabeletrain_atleast1boxcrop_underfited/best.pt\",\n",
    "    \"/kaggle/input/demining-models/yolo11_onlylabeletrain_atleast1boxcrop_underfited/yolo11_onlylabeletrain_atleast1boxcrop_underfited/last.pt\"\n",
    "]\n",
    "\n",
    "DEVICE = 'cuda:0'\n",
    "CONFIDENCE_THRESHOLD = 0.3\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# WBF Configuration\n",
    "YOLO_ENSEMBLE_WEIGHT = 1.0  # Weight for YOLO ensemble in final fusion\n",
    "MMDET_WEIGHT = 2.0          # Weight for MMDet in final fusion\n",
    "WBF_IOU_THRESHOLD = 0.5\n",
    "WBF_SKIP_BOX_THRESHOLD = 0.0001\n",
    "\n",
    "# Class mapping for submission\n",
    "CLASS_LABEL_MAPPING = {\n",
    "    0: 0,  # anti_tank_mine -> 0\n",
    "    1: 1,  # anti_personnel_mine -> 1  \n",
    "    2: 2   # other_explosive -> 2\n",
    "}\n",
    "\n",
    "# Global model instances\n",
    "_model_instance = None\n",
    "_yolo_models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1392540",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T12:08:46.709068Z",
     "iopub.status.busy": "2025-07-27T12:08:46.708848Z",
     "iopub.status.idle": "2025-07-27T12:08:46.738897Z",
     "shell.execute_reply": "2025-07-27T12:08:46.738300Z"
    },
    "papermill": {
     "duration": 0.035145,
     "end_time": "2025-07-27T12:08:46.740033",
     "exception": false,
     "start_time": "2025-07-27T12:08:46.704888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_model():\n",
    "    \"\"\"Get or initialize the MMDet model instance\"\"\"\n",
    "    global _model_instance\n",
    "    if _model_instance is None:\n",
    "        print(\"Loading MMDetection model...\")\n",
    "        _model_instance = DetInferencer(MODEL_CONFIG, CHECKPOINT, DEVICE)\n",
    "        print(\"MMDet model loaded successfully!\")\n",
    "    return _model_instance\n",
    "\n",
    "def get_yolo_models():\n",
    "    \"\"\"Get or initialize all YOLO model instances\"\"\"\n",
    "    global _yolo_models\n",
    "    if not _yolo_models:\n",
    "        print(f\"Loading {len(YOLO_MODEL_PATHS)} YOLO models...\")\n",
    "        for i, path in enumerate(YOLO_MODEL_PATHS):\n",
    "            print(f\"  Loading YOLO model {i+1}/{len(YOLO_MODEL_PATHS)}: {Path(path).name}\")\n",
    "            _yolo_models.append(YOLO(path))\n",
    "        print(\"All YOLO models loaded successfully!\")\n",
    "    return _yolo_models\n",
    "\n",
    "def preprocess_image(image_path: str, target_size: int = 640) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Load, preprocess image and return 1CHW tensor \n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the image file\n",
    "        target_size: Target size for the model input\n",
    "    \n",
    "    Returns:\n",
    "        Preprocessed image tensor\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    # Resize image\n",
    "    image = image.resize((target_size, target_size))\n",
    "    \n",
    "    # Convert to tensor and normalize\n",
    "    image_tensor = torch.from_numpy(np.array(image)).float()\n",
    "    image_tensor = image_tensor.permute(2, 0, 1) / 255.0  # HWC to CHW and normalize\n",
    "    \n",
    "    # Add batch dimension\n",
    "    image_tensor = image_tensor.unsqueeze(0)\n",
    "    \n",
    "    return image_tensor.cuda()\n",
    "\n",
    "def get_image_dimensions(image_path: str) -> Tuple[int, int]:\n",
    "    \"\"\"Get original image dimensions\"\"\"\n",
    "    try:\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            return None, None\n",
    "        height, width = img.shape[:2]\n",
    "        return width, height\n",
    "    except:\n",
    "        return None, None\n",
    "\n",
    "def normalize_bbox(bbox, img_width: int, img_height: int) -> Tuple[float, float, float, float]:\n",
    "    \"\"\"Convert absolute bbox to normalized center format\"\"\"\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    \n",
    "    # Calculate center and dimensions\n",
    "    center_x = (x1 + x2) / 2\n",
    "    center_y = (y1 + y2) / 2\n",
    "    width = x2 - x1\n",
    "    height = y2 - y1\n",
    "    \n",
    "    # Normalize to [0, 1]\n",
    "    cx_norm = center_x / img_width\n",
    "    cy_norm = center_y / img_height\n",
    "    w_norm = width / img_width\n",
    "    h_norm = height / img_height\n",
    "    \n",
    "    # Clamp to [0, 1] range\n",
    "    cx_norm = max(0, min(1, cx_norm))\n",
    "    cy_norm = max(0, min(1, cy_norm))\n",
    "    w_norm = max(0, min(1, w_norm))\n",
    "    h_norm = max(0, min(1, h_norm))\n",
    "    \n",
    "    return cx_norm, cy_norm, w_norm, h_norm\n",
    "\n",
    "def predict_batch_images(image_paths: List[str]) -> List[List[Tuple[int, float, float, float, float]]]:\n",
    "    \"\"\"\n",
    "    Run ensemble batch prediction with multiple YOLOs + MMDet + WBF\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get dimensions for valid images\n",
    "        dimensions = []\n",
    "        valid_paths = []\n",
    "        valid_indices = []\n",
    "        \n",
    "        for i, image_path in enumerate(image_paths):\n",
    "            img_width, img_height = get_image_dimensions(image_path)\n",
    "            if img_width is not None and img_height is not None:\n",
    "                dimensions.append((img_width, img_height))\n",
    "                valid_paths.append(image_path)\n",
    "                valid_indices.append(i)\n",
    "        \n",
    "        if not valid_paths:\n",
    "            return [[] for _ in image_paths]\n",
    "        \n",
    "        # Get MMDet batch predictions\n",
    "        mmdet_model = get_model()\n",
    "        mmdet_results = mmdet_model(valid_paths, out_dir=None, no_save_vis=True, no_save_pred=True)\n",
    "        \n",
    "        # Get all YOLO batch predictions\n",
    "        yolo_models = get_yolo_models()\n",
    "        all_yolo_results = []\n",
    "        for i, model in enumerate(yolo_models):\n",
    "            print(f\"Running YOLO model {i+1}/{len(yolo_models)} on batch...\")\n",
    "            yolo_results = model(valid_paths, conf=CONFIDENCE_THRESHOLD, verbose=False)\n",
    "            all_yolo_results.append(yolo_results)\n",
    "        \n",
    "        # Process each image with ensemble\n",
    "        all_predictions = [[] for _ in image_paths]\n",
    "        \n",
    "        for idx, (img_path, (img_width, img_height)) in enumerate(zip(valid_paths, dimensions)):\n",
    "            # Extract MMDet predictions\n",
    "            mmdet_pred = mmdet_results['predictions'][idx]\n",
    "            mmdet_boxes, mmdet_scores, mmdet_labels = [], [], []\n",
    "            \n",
    "            if len(mmdet_pred['scores']) > 0:\n",
    "                for bbox, score, label in zip(mmdet_pred['bboxes'], mmdet_pred['scores'], mmdet_pred['labels']):\n",
    "                    if score >= CONFIDENCE_THRESHOLD:\n",
    "                        x1, y1, x2, y2 = bbox\n",
    "                        mmdet_boxes.append([x1/img_width, y1/img_height, x2/img_width, y2/img_height])\n",
    "                        mmdet_scores.append(float(score))\n",
    "                        mmdet_labels.append(int(label))\n",
    "            \n",
    "            # Extract all YOLO predictions\n",
    "            all_yolo_boxes = []\n",
    "            all_yolo_scores = []\n",
    "            all_yolo_labels = []\n",
    "            \n",
    "            for yolo_results in all_yolo_results:\n",
    "                yolo_boxes, yolo_scores, yolo_labels = [], [], []\n",
    "                yolo_result = yolo_results[idx]\n",
    "                \n",
    "                if yolo_result.boxes is not None:\n",
    "                    for box in yolo_result.boxes:\n",
    "                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "                        yolo_boxes.append([x1/img_width, y1/img_height, x2/img_width, y2/img_height])\n",
    "                        yolo_scores.append(float(box.conf[0]))\n",
    "                        yolo_labels.append(int(box.cls[0]))\n",
    "                \n",
    "                all_yolo_boxes.append(yolo_boxes)\n",
    "                all_yolo_scores.append(yolo_scores)\n",
    "                all_yolo_labels.append(yolo_labels)\n",
    "            \n",
    "            # Stage 1: Ensemble all YOLO models first\n",
    "            if len(all_yolo_boxes) > 1:\n",
    "                # Multiple YOLOs - ensemble them first\n",
    "                yolo_weights = [1.0] * len(all_yolo_boxes)  # Equal weights for all YOLOs\n",
    "                \n",
    "                yolo_fused_boxes, yolo_fused_scores, yolo_fused_labels = weighted_boxes_fusion(\n",
    "                    all_yolo_boxes, all_yolo_scores, all_yolo_labels,\n",
    "                    weights=yolo_weights, \n",
    "                    iou_thr=WBF_IOU_THRESHOLD, \n",
    "                    skip_box_thr=WBF_SKIP_BOX_THRESHOLD\n",
    "                )\n",
    "            elif len(all_yolo_boxes) == 1:\n",
    "                # Single YOLO - use directly\n",
    "                yolo_fused_boxes = all_yolo_boxes[0]\n",
    "                yolo_fused_scores = all_yolo_scores[0] \n",
    "                yolo_fused_labels = all_yolo_labels[0]\n",
    "            else:\n",
    "                # No YOLO predictions\n",
    "                yolo_fused_boxes, yolo_fused_scores, yolo_fused_labels = [], [], []\n",
    "\n",
    "            # Stage 2: Combine YOLO ensemble with MMDet\n",
    "            final_boxes_list = [yolo_fused_boxes, mmdet_boxes]\n",
    "            final_scores_list = [yolo_fused_scores, mmdet_scores]\n",
    "            final_labels_list = [yolo_fused_labels, mmdet_labels]\n",
    "            final_weights = [YOLO_ENSEMBLE_WEIGHT, MMDET_WEIGHT]\n",
    "            \n",
    "            fused_boxes, fused_scores, fused_labels = weighted_boxes_fusion(\n",
    "                final_boxes_list, final_scores_list, final_labels_list,\n",
    "                weights=final_weights, \n",
    "                iou_thr=WBF_IOU_THRESHOLD, \n",
    "                skip_box_thr=WBF_SKIP_BOX_THRESHOLD\n",
    "            )\n",
    "            \n",
    "            # Convert to submission format\n",
    "            detected_objects = []\n",
    "            for box, score, label in zip(fused_boxes, fused_scores, fused_labels):\n",
    "                if score >= CONFIDENCE_THRESHOLD:\n",
    "                    x1, y1, x2, y2 = box\n",
    "                    cx = (x1 + x2) / 2\n",
    "                    cy = (y1 + y2) / 2\n",
    "                    w = x2 - x1\n",
    "                    h = y2 - y1\n",
    "                    \n",
    "                    submission_label = CLASS_LABEL_MAPPING.get(int(label), int(label))\n",
    "                    \n",
    "                    detected_objects.append((\n",
    "                        int(submission_label),\n",
    "                        float(cx),\n",
    "                        float(cy),\n",
    "                        float(w),\n",
    "                        float(h)\n",
    "                    ))\n",
    "            \n",
    "            original_idx = valid_indices[idx]\n",
    "            all_predictions[original_idx] = detected_objects\n",
    "        \n",
    "        return all_predictions\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Batch ensemble processing failed: {str(e)}\")\n",
    "        return [[] for _ in image_paths]\n",
    "\n",
    "def predict_single_image(image_path: str) -> List[Tuple[int, float, float, float, float]]:\n",
    "    \"\"\"\n",
    "    Fallback single image prediction (not used with batch processing)\n",
    "    \"\"\"\n",
    "    return []\n",
    "\n",
    "def get_test_images(images_folder: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Get all image files from the specified folder.\n",
    "    \n",
    "    Args:\n",
    "        images_folder: Path to the folder containing images\n",
    "    \n",
    "    Returns:\n",
    "        List of image file paths\n",
    "    \"\"\"\n",
    "    image_extensions = {'.jpg', '.jpeg', '.png'}\n",
    "    image_files = []\n",
    "    \n",
    "    for file_path in Path(images_folder).rglob('*'):\n",
    "        if file_path.suffix.lower() in image_extensions:\n",
    "            image_files.append(str(file_path))\n",
    "    \n",
    "    return sorted(image_files)\n",
    "\n",
    "def generate_submission(\n",
    "    images_folder: str = \"/kaggle/input/uadamage-demining-competition/test\",\n",
    "    output_path: str = \"submission.csv\",\n",
    "    use_batch_processing: bool = True\n",
    "):   \n",
    "    \"\"\"\n",
    "    Generate submission CSV with multi-model ensemble predictions\n",
    "    \n",
    "    Args:\n",
    "        images_folder: Path to test images folder\n",
    "        output_path: Output CSV file path\n",
    "        use_batch_processing: Whether to use batch processing for speed\n",
    "    \"\"\"\n",
    "    print(f\"🔍 Scanning images in: {images_folder}\")\n",
    "    image_files = get_test_images(images_folder)\n",
    "    \n",
    "    if not image_files:\n",
    "        raise ValueError(f\"No image files found in {images_folder}\")\n",
    "    \n",
    "    print(f\"📁 Found {len(image_files)} images\")\n",
    "    print(f\"⚙️ Using batch processing: {use_batch_processing}\")\n",
    "    print(f\"📦 Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"🎯 Confidence threshold: {CONFIDENCE_THRESHOLD}\")\n",
    "    print(f\"🤖 Using {len(YOLO_MODEL_PATHS)} YOLO models -> YOLO ensemble -> MMDetection with WBF\")\n",
    "    print(f\"⚖️ Final weights: [YOLO_ensemble: {YOLO_ENSEMBLE_WEIGHT}, MMDet: {MMDET_WEIGHT}]\")\n",
    "    \n",
    "    # Prepare data for CSV\n",
    "    rows = []\n",
    "    row_id = 1\n",
    "    \n",
    "    if use_batch_processing and len(image_files) > 1:\n",
    "        # Process images in batches\n",
    "        num_batches = math.ceil(len(image_files) / BATCH_SIZE)\n",
    "        print(f\"🚀 Processing {len(image_files)} images in {num_batches} batches...\")\n",
    "        \n",
    "        for batch_idx in tqdm(range(num_batches), desc=\"Processing multi-model ensemble batches\"):\n",
    "            # Get batch of images\n",
    "            start_idx = batch_idx * BATCH_SIZE\n",
    "            end_idx = min(start_idx + BATCH_SIZE, len(image_files))\n",
    "            batch_image_files = image_files[start_idx:end_idx]\n",
    "            \n",
    "            # Run batch prediction\n",
    "            batch_predictions = predict_batch_images(batch_image_files)\n",
    "            \n",
    "            # Process results for this batch\n",
    "            for image_path, predictions in zip(batch_image_files, batch_predictions):\n",
    "                # Extract scene ID from filename (remove extension)\n",
    "                scene_id = Path(image_path).stem\n",
    "                \n",
    "                if predictions:\n",
    "                    # Add predictions for this scene\n",
    "                    for label, cx, cy, w, h in predictions:\n",
    "                        rows.append({\n",
    "                            'id': row_id,\n",
    "                            'scene': scene_id,\n",
    "                            'label': label,\n",
    "                            'cx': round(cx, 6),\n",
    "                            'cy': round(cy, 6),\n",
    "                            'w': round(w, 6),\n",
    "                            'h': round(h, 6)\n",
    "                        })\n",
    "                        row_id += 1\n",
    "                else:\n",
    "                    # No predictions - add empty scene marker\n",
    "                    rows.append({\n",
    "                        'id': row_id,\n",
    "                        'scene': scene_id,\n",
    "                        'label': -1,\n",
    "                        'cx': -1,\n",
    "                        'cy': -1,\n",
    "                        'w': -1,\n",
    "                        'h': -1\n",
    "                    })\n",
    "                    row_id += 1\n",
    "    else:\n",
    "        # Process images one by one (fallback)\n",
    "        print(\"🔄 Processing images individually...\")\n",
    "        for image_path in tqdm(image_files, desc=\"Processing images\"):\n",
    "            # Extract scene ID from filename (remove extension)\n",
    "            scene_id = Path(image_path).stem\n",
    "            \n",
    "            # Run prediction\n",
    "            predictions = predict_single_image(image_path)\n",
    "            \n",
    "            if predictions:\n",
    "                # Add predictions for this scene\n",
    "                for label, cx, cy, w, h in predictions:\n",
    "                    rows.append({\n",
    "                        'id': row_id,\n",
    "                        'scene': scene_id,\n",
    "                        'label': label,\n",
    "                        'cx': round(cx, 6),\n",
    "                        'cy': round(cy, 6),\n",
    "                        'w': round(w, 6),\n",
    "                        'h': round(h, 6)\n",
    "                    })\n",
    "                    row_id += 1\n",
    "            else:\n",
    "                # No predictions - add empty scene marker\n",
    "                rows.append({\n",
    "                    'id': row_id,\n",
    "                    'scene': scene_id,\n",
    "                    'label': -1,\n",
    "                    'cx': -1,\n",
    "                    'cy': -1,\n",
    "                    'w': -1,\n",
    "                    'h': -1\n",
    "                })\n",
    "                row_id += 1\n",
    "\n",
    "    # Create DataFrame and save to CSV\n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    # Validate submission format\n",
    "    print(\"✅ Validating submission format...\")\n",
    "    \n",
    "    # Check required columns\n",
    "    required_cols = ['id', 'scene', 'label', 'cx', 'cy', 'w', 'h']\n",
    "    assert all(col in df.columns for col in required_cols), \"Missing required columns\"\n",
    "    \n",
    "    # Check data types and ranges\n",
    "    df['id'] = df['id'].astype(int)\n",
    "    df['label'] = df['label'].astype(int)\n",
    "    df['cx'] = df['cx'].astype(float)\n",
    "    df['cy'] = df['cy'].astype(float)\n",
    "    df['w'] = df['w'].astype(float)\n",
    "    df['h'] = df['h'].astype(float)\n",
    "    \n",
    "    # Check coordinate ranges\n",
    "    coord_cols = ['cx', 'cy', 'w', 'h']\n",
    "    for col in coord_cols:\n",
    "        non_empty = df[df[col] != -1][col]\n",
    "        if len(non_empty) > 0:\n",
    "            assert non_empty.min() >= 0 and non_empty.max() <= 1, f\"{col} values out of range [0,1]\"\n",
    "    \n",
    "    # Check label values\n",
    "    valid_labels = {-1, 0, 1, 2}\n",
    "    assert set(df['label'].unique()).issubset(valid_labels), \"Invalid label values found\"\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"💾 Solution saved to: {output_path}\")\n",
    "    print(f\"📊 Total predictions: {len(df)}\")\n",
    "    \n",
    "    # Print statistics\n",
    "    detections = df[df['label'] != -1]\n",
    "    empty_scenes = df[df['label'] == -1]\n",
    "    \n",
    "    print(f\"\\\\n📈 Submission Statistics:\")\n",
    "    print(f\"   Total rows: {len(df)}\")\n",
    "    print(f\"   Unique scenes: {df['scene'].nunique()}\")\n",
    "    print(f\"   Detections: {len(detections)}\")\n",
    "    print(f\"   Empty scenes: {len(empty_scenes)}\")\n",
    "    print(f\"   Detection rate: {((df['scene'].nunique() - len(empty_scenes)) / df['scene'].nunique() * 100):.1f}%\")\n",
    "    \n",
    "    if len(detections) > 0:\n",
    "        label_counts = detections['label'].value_counts().sort_index()\n",
    "        print(f\"\\\\n🏷️ Class Distribution:\")\n",
    "        class_names = {0: \"Anti-tank mines\", 1: \"Anti-personnel mines\", 2: \"Other explosives\"}\n",
    "        for label, count in label_counts.items():\n",
    "            print(f\"   Class {label} ({class_names.get(label, 'Unknown')}): {count}\")\n",
    "    \n",
    "    print(f\"\\\\n🎯 Ready for Kaggle submission!\")\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89f05ebf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T12:08:46.747049Z",
     "iopub.status.busy": "2025-07-27T12:08:46.746701Z",
     "iopub.status.idle": "2025-07-27T12:08:54.728852Z",
     "shell.execute_reply": "2025-07-27T12:08:54.727747Z"
    },
    "papermill": {
     "duration": 7.987327,
     "end_time": "2025-07-27T12:08:54.730396",
     "exception": false,
     "start_time": "2025-07-27T12:08:46.743069",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Scanning images in: /kaggle/input/uadamage-demining-competition/test\n",
      "📁 Found 4 images\n",
      "⚙️ Using batch processing: True\n",
      "📦 Batch size: 16\n",
      "🎯 Confidence threshold: 0.3\n",
      "🤖 Using 4 YOLO models -> YOLO ensemble -> MMDetection with WBF\n",
      "⚖️ Final weights: [YOLO_ensemble: 1.0, MMDet: 2.0]\n",
      "🚀 Processing 4 images in 1 batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing multi-model ensemble batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MMDetection model...\n",
      "Loads checkpoint by local backend from path: /kaggle/input/ai-jam-inference/best_coco_bbox_mAP_epoch_19_semifixedv2.pth\n",
      "07/27 12:08:50 - mmengine - WARNING - Failed to search registry with scope \"mmdet\" in the \"function\" registry tree. As a workaround, the current \"function\" registry in \"mmengine\" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether \"mmdet\" is a correct scope, or whether the registry is initialized.\n",
      "07/27 12:08:50 - mmengine - WARNING - `Visualizer` backend is not initialized because save_dir is None.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b149563c02f74274876f293c2675781a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMDet model loaded successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming \n",
       "release, it will be required to pass the indexing argument. (Triggered internally at  \n",
       "../aten/src/ATen/native/TensorShape.cpp:2894.)\n",
       "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming \n",
       "release, it will be required to pass the indexing argument. (Triggered internally at  \n",
       "../aten/src/ATen/native/TensorShape.cpp:2894.)\n",
       "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 4 YOLO models...\n",
      "  Loading YOLO model 1/4: best.pt\n",
      "  Loading YOLO model 2/4: last.pt\n",
      "  Loading YOLO model 3/4: best.pt\n",
      "  Loading YOLO model 4/4: last.pt\n",
      "All YOLO models loaded successfully!\n",
      "Running YOLO model 1/4 on batch...\n",
      "Running YOLO model 2/4 on batch...\n",
      "Running YOLO model 3/4 on batch...\n",
      "Running YOLO model 4/4 on batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing multi-model ensemble batches: 100%|██████████| 1/1 [00:07<00:00,  7.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Validating submission format...\n",
      "💾 Solution saved to: submission.csv\n",
      "📊 Total predictions: 4\n",
      "\\n📈 Submission Statistics:\n",
      "   Total rows: 4\n",
      "   Unique scenes: 4\n",
      "   Detections: 1\n",
      "   Empty scenes: 3\n",
      "   Detection rate: 25.0%\n",
      "\\n🏷️ Class Distribution:\n",
      "   Class 0 (Anti-tank mines): 1\n",
      "\\n🎯 Ready for Kaggle submission!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate submission with multi-model ensemble batch processing\n",
    "    df = generate_submission(\n",
    "        use_batch_processing=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "329b6add",
   "metadata": {
    "_cell_guid": "1cbb96bc-5e59-4b4b-b582-7881d3fd8bca",
    "_uuid": "327d2064-eeb7-4f76-b7b4-b62eef82a2d5",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-27T12:08:54.771409Z",
     "iopub.status.busy": "2025-07-27T12:08:54.771164Z",
     "iopub.status.idle": "2025-07-27T12:08:54.784862Z",
     "shell.execute_reply": "2025-07-27T12:08:54.784072Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.049466,
     "end_time": "2025-07-27T12:08:54.786166",
     "exception": false,
     "start_time": "2025-07-27T12:08:54.736700",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>scene</th>\n",
       "      <th>label</th>\n",
       "      <th>cx</th>\n",
       "      <th>cy</th>\n",
       "      <th>w</th>\n",
       "      <th>h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>000eda45a4f594e86867516ffea98533</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0d5849518ef6d6692cb63435aa50edaa</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0eb1a9e6e37eb7413cfe29f0e9d3b69f</td>\n",
       "      <td>0</td>\n",
       "      <td>0.852081</td>\n",
       "      <td>0.843308</td>\n",
       "      <td>0.135244</td>\n",
       "      <td>0.313385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0eb1e6262e21225e1fcdd8a6198af484</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                             scene  label        cx        cy         w  \\\n",
       "0   1  000eda45a4f594e86867516ffea98533     -1 -1.000000 -1.000000 -1.000000   \n",
       "1   2  0d5849518ef6d6692cb63435aa50edaa     -1 -1.000000 -1.000000 -1.000000   \n",
       "2   3  0eb1a9e6e37eb7413cfe29f0e9d3b69f      0  0.852081  0.843308  0.135244   \n",
       "3   4  0eb1e6262e21225e1fcdd8a6198af484     -1 -1.000000 -1.000000 -1.000000   \n",
       "\n",
       "          h  \n",
       "0 -1.000000  \n",
       "1 -1.000000  \n",
       "2  0.313385  \n",
       "3 -1.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"/kaggle/working/submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11062df",
   "metadata": {
    "_cell_guid": "51154fb7-2c46-4209-9c93-6de269c190bc",
    "_uuid": "9be838ad-7406-424a-8369-a787f88ded8d",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.004233,
     "end_time": "2025-07-27T12:08:54.794960",
     "exception": false,
     "start_time": "2025-07-27T12:08:54.790727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 13166279,
     "sourceId": 105247,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 13186077,
     "datasetId": 7946396,
     "sourceId": 12590351,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 13184746,
     "datasetId": 7949065,
     "sourceId": 12589191,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 6141924,
     "datasetId": 3470137,
     "sourceId": 6063590,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 6141366,
     "datasetId": 3469773,
     "sourceId": 6063037,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 13185783,
     "datasetId": 7951634,
     "sourceId": 12590102,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 252601406,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 176.268756,
   "end_time": "2025-07-27T12:08:56.319023",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-27T12:06:00.050267",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0aed35ea16e1485aa1f0c5a9c390e532": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b149563c02f74274876f293c2675781a": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/output",
       "_model_module_version": "1.0.0",
       "_model_name": "OutputModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/output",
       "_view_module_version": "1.0.0",
       "_view_name": "OutputView",
       "layout": "IPY_MODEL_0aed35ea16e1485aa1f0c5a9c390e532",
       "msg_id": "",
       "outputs": [
        {
         "data": {
          "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Inference <span style=\"color: #3e393b; text-decoration-color: #3e393b\">━</span><span style=\"color: #4c383f; text-decoration-color: #4c383f\">━</span><span style=\"color: #613545; text-decoration-color: #613545\">━</span><span style=\"color: #7b334d; text-decoration-color: #7b334d\">━</span><span style=\"color: #993056; text-decoration-color: #993056\">━</span><span style=\"color: #b72c5e; text-decoration-color: #b72c5e\">━</span><span style=\"color: #d12a66; text-decoration-color: #d12a66\">━</span><span style=\"color: #e6276c; text-decoration-color: #e6276c\">━</span><span style=\"color: #f42670; text-decoration-color: #f42670\">━</span><span style=\"color: #f92672; text-decoration-color: #f92672\">━</span><span style=\"color: #f42670; text-decoration-color: #f42670\">━</span><span style=\"color: #e6276c; text-decoration-color: #e6276c\">━</span><span style=\"color: #d12a66; text-decoration-color: #d12a66\">━</span><span style=\"color: #b72c5e; text-decoration-color: #b72c5e\">━</span><span style=\"color: #993056; text-decoration-color: #993056\">━</span><span style=\"color: #7b334d; text-decoration-color: #7b334d\">━</span><span style=\"color: #613545; text-decoration-color: #613545\">━</span><span style=\"color: #4c383f; text-decoration-color: #4c383f\">━</span><span style=\"color: #3e393b; text-decoration-color: #3e393b\">━</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━</span><span style=\"color: #3e393b; text-decoration-color: #3e393b\">━</span><span style=\"color: #4c383f; text-decoration-color: #4c383f\">━</span><span style=\"color: #613545; text-decoration-color: #613545\">━</span><span style=\"color: #7b334d; text-decoration-color: #7b334d\">━</span><span style=\"color: #993056; text-decoration-color: #993056\">━</span><span style=\"color: #b72c5e; text-decoration-color: #b72c5e\">━</span><span style=\"color: #d12a66; text-decoration-color: #d12a66\">━</span><span style=\"color: #e6276c; text-decoration-color: #e6276c\">━</span><span style=\"color: #f42670; text-decoration-color: #f42670\">━</span><span style=\"color: #f92672; text-decoration-color: #f92672\">━</span><span style=\"color: #f42670; text-decoration-color: #f42670\">━</span><span style=\"color: #e6276c; text-decoration-color: #e6276c\">━</span><span style=\"color: #d12a66; text-decoration-color: #d12a66\">━</span><span style=\"color: #b72c5e; text-decoration-color: #b72c5e\">━</span><span style=\"color: #993056; text-decoration-color: #993056\">━</span><span style=\"color: #7b334d; text-decoration-color: #7b334d\">━</span><span style=\"color: #613545; text-decoration-color: #613545\">━</span><span style=\"color: #4c383f; text-decoration-color: #4c383f\">━</span><span style=\"color: #3e393b; text-decoration-color: #3e393b\">━</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━</span> <span style=\"color: #800080; text-decoration-color: #800080\">38.3 it/s</span> <span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n</pre>\n",
          "text/plain": "Inference \u001b[38;2;62;57;59m━\u001b[0m\u001b[38;2;76;56;63m━\u001b[0m\u001b[38;2;97;53;69m━\u001b[0m\u001b[38;2;123;51;77m━\u001b[0m\u001b[38;2;153;48;86m━\u001b[0m\u001b[38;2;183;44;94m━\u001b[0m\u001b[38;2;209;42;102m━\u001b[0m\u001b[38;2;230;39;108m━\u001b[0m\u001b[38;2;244;38;112m━\u001b[0m\u001b[38;2;249;38;114m━\u001b[0m\u001b[38;2;244;38;112m━\u001b[0m\u001b[38;2;230;39;108m━\u001b[0m\u001b[38;2;209;42;102m━\u001b[0m\u001b[38;2;183;44;94m━\u001b[0m\u001b[38;2;153;48;86m━\u001b[0m\u001b[38;2;123;51;77m━\u001b[0m\u001b[38;2;97;53;69m━\u001b[0m\u001b[38;2;76;56;63m━\u001b[0m\u001b[38;2;62;57;59m━\u001b[0m\u001b[38;2;58;58;58m━\u001b[0m\u001b[38;2;62;57;59m━\u001b[0m\u001b[38;2;76;56;63m━\u001b[0m\u001b[38;2;97;53;69m━\u001b[0m\u001b[38;2;123;51;77m━\u001b[0m\u001b[38;2;153;48;86m━\u001b[0m\u001b[38;2;183;44;94m━\u001b[0m\u001b[38;2;209;42;102m━\u001b[0m\u001b[38;2;230;39;108m━\u001b[0m\u001b[38;2;244;38;112m━\u001b[0m\u001b[38;2;249;38;114m━\u001b[0m\u001b[38;2;244;38;112m━\u001b[0m\u001b[38;2;230;39;108m━\u001b[0m\u001b[38;2;209;42;102m━\u001b[0m\u001b[38;2;183;44;94m━\u001b[0m\u001b[38;2;153;48;86m━\u001b[0m\u001b[38;2;123;51;77m━\u001b[0m\u001b[38;2;97;53;69m━\u001b[0m\u001b[38;2;76;56;63m━\u001b[0m\u001b[38;2;62;57;59m━\u001b[0m\u001b[38;2;58;58;58m━\u001b[0m \u001b[35m38.3 it/s\u001b[0m \u001b[36m \u001b[0m\n"
         },
         "metadata": {},
         "output_type": "display_data"
        }
       ],
       "tabbable": null,
       "tooltip": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
